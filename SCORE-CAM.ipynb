{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff25f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blacbox import Saliency\n",
    "from blacbox import RaceClassifier\n",
    "from blacbox import GuidedBackPropagation\n",
    "from blacbox import GCAM\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94405e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398588b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RaceClassifier(\"/home/dazedtiara6667/GitHub/CV/EthinicAPI/fair_face_models/res34_fair_align_multi_7_20190809.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4137ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreCAM(GCAM):\n",
    "    def __init__(self, model, interpolate = 'bilinear', device = 'cpu'):\n",
    "        super().__init__(model, interpolate, device)\n",
    "        \n",
    "    def retrieve_gcams(self, images, class_idx, key, colormap, apply_cmap = True):\n",
    "        \"\"\"\n",
    "        Retrieving gcams from the images provided\n",
    "        \"\"\"\n",
    "        gcams = []            \n",
    "        # Selecting the index \n",
    "        \n",
    "        for image in images:\n",
    "            # Taking the images\n",
    "            self.image = image.unsqueeze(0).to(self.device)\n",
    "            hadmard_products = []\n",
    "            self.image.requires_grad = True\n",
    "            output = self.model(self.image)\n",
    "            output = F.softmax(output)\n",
    "            if(class_idx == \"keepmax\"):\n",
    "                class_idx = output.argmax(dim = 1)\n",
    "            elif(class_idx == \"keepmin\"):\n",
    "                class_idx = output.argmin(dim = 1)\n",
    "\n",
    "            # Calculating gradients w.r.t to the idx selected\n",
    "            cr = output[0,class_idx]\n",
    "            cr.backward()\n",
    "                \n",
    "\n",
    "            # Retrieving gradients and activation maps\n",
    "            fmaps = self.activations[key]\n",
    "            with torch.no_grad():\n",
    "                for k in range(fmaps.shape[1]):\n",
    "                    k_map = fmaps[:,k,:,:]\n",
    "                    if k_map.max() == k_map.min():\n",
    "                        continue\n",
    "                    k_map = k_map.unsqueeze(0)\n",
    "                    #print(k_map.shape)\n",
    "                    k_map = F.interpolate(\n",
    "                        k_map, self.image.shape[2:4], mode=self.interpolate, align_corners=False\n",
    "                    )\n",
    "                    k_map = k_map - k_map.min()/(k_map.max() - k_map.min())\n",
    "                    #print(k_map.shape)\n",
    "                    #print(self.image.shape)\n",
    "                    hadmard_products.append(torch.mul(self.image,k_map))\n",
    "                    #print(hadmard_products[-1].shape)\n",
    "\n",
    "                hadmard_products = torch.cat(hadmard_products, dim = 0)\n",
    "                print(hadmard_products.shape)\n",
    "                target_scores = self.model(hadmard_products)\n",
    "                print(target_scores.shape)\n",
    "                cic_scores = torch.sub(target_scores[:,class_idx], cr)\n",
    "                alpha = F.softmax(cic_scores)\n",
    "                score_cam = torch.mul(self.activations[key], alpha).sum(dim = 1, keepdim = True)\n",
    "                print(score_cam.shape)\n",
    "                #print(alpha.shape)\n",
    "                #print(self.activations[key].shape)\n",
    "                #score_cam = F.relu(alpha*self.activations[key]).sum(dim = 1, keepdim = True)\n",
    "                score_cam = F.interpolate(\n",
    "                    score_cam, self.image.shape[2:4], mode=self.interpolate, align_corners=False\n",
    "                )\n",
    "                print(score_cam.shape)\n",
    "\n",
    "                score_cam = score_cam.cpu().squeeze(0).detach().permute(1,2,0).numpy()\n",
    "                score_cam = cv2.normalize(\n",
    "                    score_cam, \n",
    "                    None,\n",
    "                    alpha = 0,\n",
    "                    beta = 255,\n",
    "                    norm_type = cv2.NORM_MINMAX,\n",
    "                    dtype = cv2.CV_8UC3\n",
    "                )\n",
    "                score_cam = cv2.applyColorMap(score_cam, self.colormap_dict[colormap])\n",
    "\n",
    "                gcams.append(score_cam)\n",
    "        \n",
    "        # Removing the hooks, activations, gradients stored\n",
    "        self.ac_handler.remove()\n",
    "        self.grad_handler.remove()\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "        return np.array(gcams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ed8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = ScoreCAM(clf.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c19d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.colormap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv2.imread('blacbox/architectures/images/black.png')\n",
    "image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "image2 = cv2.imread('blacbox/architectures/images/dog.png')\n",
    "image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa53d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "        transform = transforms.Compose([\n",
    "                        \n",
    "                        transforms.ToPILImage(),\n",
    "                        transforms.Resize((224, 224)),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "        ])\n",
    "\n",
    "        image = transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = preprocess_image(image1)\n",
    "image2 = preprocess_image(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31d8fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.stack((image1, image2), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f01eabd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 3, 224, 224])\n",
      "torch.Size([512, 18])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (512) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a13e4097748f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mheatmaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreveal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'keepmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bone'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/GitHub/CV/blacbox/blacbox/activation_visualizers/gcam.py\u001b[0m in \u001b[0;36mreveal\u001b[0;34m(self, images, module, class_idx, path, colormap)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mac_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mgcams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_gcams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgcams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-255088ecad1d>\u001b[0m in \u001b[0;36mretrieve_gcams\u001b[0;34m(self, images, class_idx, key, colormap, apply_cmap)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mcic_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcic_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mscore_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_cam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m#print(alpha.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (512) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "heatmaps = cam.reveal(images = images, module = clf.model.layer4[0].conv1, class_idx = 'keepmax', colormap = 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d4c5aa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7, 512, 7, 7)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bfe2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
